{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Easy natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Quick rundown of NLP taks\n",
    "* Build your own spam detector\n",
    "* Build your own sentiment analyzer\n",
    "* NLTK library exploration\n",
    "* Latent semantic analysis (LSA)\n",
    "* Build your own article spinner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very good @"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Spam detection**\n",
    "    - Filtering spam emails in inbox and/or categorizing them like \"Primary\", \"Social\" etc.\n",
    "\n",
    "\n",
    "* **POS (parts-of-speech) tagging**\n",
    "    - Given a sentence, we identify noun, adjective, verb etc.\n",
    "\n",
    "\n",
    "* **NER (named-entity recognition)**\n",
    "    - Given a sentence, we identify whether the word represents a person, an organization etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty good @"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sentiment analysis**\n",
    "    - Assigning a score to a sentence based on word analysis.\n",
    "    \n",
    "    \n",
    "* **Machine Translation**\n",
    "    - Translating text into different languages\n",
    "    \n",
    "    \n",
    "* **Information Extraction**\n",
    "    - E.g. Adding events to the calender by automatically reading content of message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needs Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Machine conversations**\n",
    "    - Speech recognition (Cortana, Siri), extracting meaning from spoken words and replying with meaningful response.\n",
    "\n",
    "\n",
    "* **Paraphrasing and summarization**\n",
    "    - Summarize an article into few sentences (AI base AMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is NLP hard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguity 1\n",
    "\n",
    "Republicans Grill IRS Chief Over Lost Emails\n",
    "\n",
    "Interpretations:\n",
    "\n",
    "1. Republicans harshly question the chief about the emails\n",
    "2. Republicans cook the chief using email as the fuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguity 2\n",
    "\n",
    "I saw a man on a hill with a telescope.\n",
    "\n",
    "Interpretations:\n",
    "\n",
    "1. There's a man on a hill, and I'm watching him with my telescope.\n",
    "2. There's a man on a hill, who I'm seeing, and he has a telescope.\n",
    "3. There's a man, and he's on a hill that also has a telescope on it.\n",
    "4. I'm on a hill, and I saw a man using a telescope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguity 3\n",
    "\n",
    "Twitter feeds: 'u', 'ur', 'lol', 'netflix and chill'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building a spam detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: [https://archive.ics.uci.edu/ml/datasets/Spambase]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Columns 1 - 48: word frequency measure** - number of times a word appears divided by number of words in document * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last column is label**: 1 = spam, 0 = not spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9  ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "\n",
       "    50     51     52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('spambase/spambase.data', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.as_matrix()\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[:,:48]\n",
    "Y = data[:,-1]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3220, 48)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1381, 48)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3220,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1381,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for MultinomialNB: 0.8725561187545257\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)\n",
    "print('Classification rate for MultinomialNB: {}'.format(model.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for AdaBoost: 0.9261404779145547\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "print('Classification rate for AdaBoost: {}'.format(model.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on feature extraction (getting the features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collectively called as **\"bag of words\"** we have following techiniques available for extracing features from words:\n",
    "\n",
    "1. word proportion (the one used in above example)\n",
    "2. raw word counts\n",
    "3. binary (1 if word appears, 0 otherwise)\n",
    "4. TF-IDF (takes into account the face that some words appear in many documents and hence don't really tell us much e.g. 'and', 'in', 'or' etc.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful link for TF-IDF [http://scikit-learn.org/stable/modules/feature_extraction.html]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a sentiment analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sentiment = how positive or negative some text is\n",
    "* applications = amazon reviews, yelp reviews, hotel reviews, tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: [https://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of our sentiment analyzer\n",
    "\n",
    "* We'll just look at the **electronics category**, but you can try the same code on others\n",
    "* We could use 5 start targets to do regression, but let's just do classification since they are already marked 'positive' and 'negative'\n",
    "* **XML parser (BeautifulSoup)**\n",
    "* Only look at key **'review_text'**\n",
    "* We'll need 2 passes, one to determine vocabulary size and which index corresponds to which word, and one to create data vectors\n",
    "* After that, we can just use any **SKLearn classifier** as we did previously\n",
    "* But we'll **use logistic regression so we can interpret the weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====================================================================\n",
    "* Essential imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initilizing WordNetLemmatizer\n",
    "* Initializing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " 'she',\n",
       " 'should',\n",
       " 'shouldn',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " 'wouldn',\n",
       " 'y',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts words to their base form\n",
    "# More on lemmatizer\n",
    "# http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization \n",
    "\n",
    "word_net_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# reading 'given' stopwords, words that give neutral sentiment\n",
    "# rstrip - removes the characters to the right based on given parameter\n",
    "#stopwords = set(word.rstrip() for word in open('stopwords.txt'))\n",
    "\n",
    "# we will use nltk provided stopwords\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More on BeautifulSoup\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "# reading XML files using BeautifulSoup (using the lxml parser)\n",
    "positive_reviews = BeautifulSoup(open('electronics/positive.review').read(), 'lxml')\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review').read(), 'lxml')\n",
    "negative_reviews = negative_reviews.findAll('review_text')\n",
    "\n",
    "type(positive_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Shuffling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffling data\n",
    "np.random.shuffle(positive_reviews)\n",
    "np.random.shuffle(negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Checking dataset lengths, equalizing datasets in case of inequal lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equalizing labels\n",
    "# positive_reviews = positive_reviews[:len(negative_reviews)]\n",
    "len(positive_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initializing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating the main vocabulary dictionary\n",
    "vocabulary = {}\n",
    "vocabulary_index = 0\n",
    "\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define custom tokenizer that does the following:\n",
    "    - Lowers all string\n",
    "    - Tokenize string using nltk.tokenize\n",
    "    - Remove any tokens(words) of length <= 2\n",
    "    - Lemmatize tokens(words)\n",
    "    - Filter tokens(words) with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining custom tokenizing function\n",
    "def custom_tokenizer(s):\n",
    "    # lowering all text\n",
    "    s = s.lower()\n",
    "    \n",
    "    # this is equivalent to list.split() but let's keep faith in word_tokenize()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    \n",
    "    # remove the words with length <= 2\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    # lemmatize the tokens\n",
    "    tokens = list(map(lambda token: word_net_lemmatizer.lemmatize(token), tokens))\n",
    "    \n",
    "    # remove the stopwords from the tokens\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()\n",
    "# tokens = nltk.tokenize.word_tokenize('This is to certify that Milind Dalvi successfully completed 13 hours of Complete Python Bootcamp: Go from zero to hero in Python online course on Aug. 28, 2017')\n",
    "# # remove the words with length <= 2\n",
    "# tokens = [token for token in tokens if len(token) > 2]\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #lemmatize the tokens\n",
    "# tokens = list(map(lambda token: word_net_lemmatizer.lemmatize(token), tokens))\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cycle positive and negative reviews and do the following:\n",
    "    - Tokenize reviews (using custom tokenizer)\n",
    "    - Append tokens to respective list (positive or negative) (this list will house all tokens lists)\n",
    "    - Index token into mail **'vocabulary'** with unique **'vocabulary_index'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['second', 'g15', 'keyboard', \"'ve\", 'purchased', 'first', 'one', 'broke', 'great', 'keyboard', 'keep', 'one', 'home', 'take', 'one', 'lanning', 'issue', 'keyboard', 'lcd', 'pretty', 'useless', 'fast', 'paced', 'fps', 'game', 'otherwise', 'good', 'keeping', 'clock', 'one', 'doe', \"n't\", 'lose', 'track', 'time']\n",
      "[\"'ve\", 'mp3', 'player', 'since', 'came', 'anyone', 'still', 'considering', 'getting', 'must', 'advise', 'otherwise', 'mp3', 'player', 'look', 'feel', 'great', 'navigational', 'feature', 'easy', 'use', 'problem', 'player', 'extremely', 'unreliable', 'used', 'version', 'player', 'expecting', 'time', 'player', 'wa', 'simply', 'faulty', 'glitch', 'terribly', 'constantly', 'take', 'battery', 'reset', 'player', 'freeze', 'normal', 'play', 'also', 'freeze', 'half', 'time', 'try', 'sync', 'wmp', 'addition', 'sometimes', 'simply', 'refuse', 'take', 'deleted', 'song', 'h10', 'list', 'track', 'even', 'though', 'longer', 'exist', 'player', 'finally', 'iriver', 'ha', 'terrible', 'customer', 'service', 'even', 'back', 'product', 'owner', 'two', 'irivers', 'h10', '120', 'seen', 'function', 'offer', 'update', 'faulty', 'firmware', 'month', 'leave', 'product', 'produce', 'new', 'one', 'seems', 'like', 'new', 'mp3', 'player', 'introduced', 'every', 'month', 'customer', 'service', 'extremely', 'hard', 'get', 'hold', 'simply', 'say', 'need', 'send', 'factory', 'occasionally', 'get', 'rep', 'actually', 'send', 'brand', 'new', 'one', 'free', 'though', 'smart', 'stick', 'creative', 'new', 'toshiba', 'gigabeat', 'clench', 'fist', 'ipod']\n"
     ]
    }
   ],
   "source": [
    "for review in positive_reviews:\n",
    "    tokens = custom_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = vocabulary_index\n",
    "            vocabulary_index += 1\n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = custom_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = vocabulary_index\n",
    "            vocabulary_index += 1\n",
    "\n",
    "print(positive_tokenized[0])\n",
    "print(negative_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set **N - Number of records'** equals to len(pos) + len(neg) token lists\n",
    "* Create **data** matrix for all data equals to **N** records and **+1 column-wise** for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records N\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "data = np.zeros((N, len(vocabulary) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define custom tokens_to_vector function as below:\n",
    "    - Create a zeros vector of lenght (vocabulary + 1) (+1 for target labels)\n",
    "    - Increment those value indexes based on vocabulary found in provided token list.\n",
    "    - Label the target label\n",
    "    - Return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining token to vector function\n",
    "def tokens_to_vector(tokens_list, label):\n",
    "    # this is the feature vector with target label\n",
    "    feature = np.zeros(len(vocabulary) + 1)\n",
    "    for token in tokens_list:\n",
    "        feature[vocabulary[token]] += 1\n",
    "    feature = (feature * 100.0) / feature.sum()\n",
    "    feature[-1] = label\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert tokens to vectors and make data\n",
    "* Preview to be saved as CSV using Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_0</th>\n",
       "      <th>input_1</th>\n",
       "      <th>input_2</th>\n",
       "      <th>input_3</th>\n",
       "      <th>input_4</th>\n",
       "      <th>input_5</th>\n",
       "      <th>input_6</th>\n",
       "      <th>input_7</th>\n",
       "      <th>input_8</th>\n",
       "      <th>input_9</th>\n",
       "      <th>...</th>\n",
       "      <th>input_11284</th>\n",
       "      <th>input_11285</th>\n",
       "      <th>input_11286</th>\n",
       "      <th>input_11287</th>\n",
       "      <th>input_11288</th>\n",
       "      <th>input_11289</th>\n",
       "      <th>input_11290</th>\n",
       "      <th>input_11291</th>\n",
       "      <th>input_11292</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>11.428571</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.325581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.325581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.325581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.169591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.169591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11294 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    input_0   input_1   input_2   input_3   input_4   input_5    input_6  \\\n",
       "0  2.857143  2.857143  8.571429  2.857143  2.857143  2.857143  11.428571   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "3  0.000000  0.000000  0.000000  2.325581  0.000000  0.000000   2.325581   \n",
       "4  1.169591  0.000000  0.000000  1.169591  0.000000  0.000000   0.584795   \n",
       "\n",
       "    input_7   input_8   input_9   ...    input_11284  input_11285  \\\n",
       "0  2.857143  2.857143  2.857143   ...            0.0          0.0   \n",
       "1  0.000000  0.990099  0.000000   ...            0.0          0.0   \n",
       "2  0.000000  0.000000  0.000000   ...            0.0          0.0   \n",
       "3  0.000000  2.325581  0.000000   ...            0.0          0.0   \n",
       "4  0.000000  0.000000  0.584795   ...            0.0          0.0   \n",
       "\n",
       "   input_11286  input_11287  input_11288  input_11289  input_11290  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0   \n",
       "1          0.0          0.0          0.0          0.0          0.0   \n",
       "2          0.0          0.0          0.0          0.0          0.0   \n",
       "3          0.0          0.0          0.0          0.0          0.0   \n",
       "4          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   input_11291  input_11292  output  \n",
       "0          0.0          0.0     1.0  \n",
       "1          0.0          0.0     1.0  \n",
       "2          0.0          0.0     1.0  \n",
       "3          0.0          0.0     1.0  \n",
       "4          0.0          0.0     1.0  \n",
       "\n",
       "[5 rows x 11294 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for tokens_list in positive_tokenized:\n",
    "    XY = tokens_to_vector(tokens_list, 1)\n",
    "    data[i, :] = XY\n",
    "    i += 1\n",
    "    \n",
    "for tokens_list in negative_tokenized:\n",
    "    XY = tokens_to_vector(tokens_list, 0)\n",
    "    data[i, :] = XY\n",
    "    i += 1\n",
    "    \n",
    "df = pd.DataFrame(data=data, columns=['input_' + str(i) for i in range(len(vocabulary))] + ['output'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :-1]\n",
    "Y = data[:,-1]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate: 0.7933333333333333\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "print('Classification rate: {}'.format(model.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great 0.562401041197\n",
      "back -0.682942936312\n",
      "bit 0.522290229473\n",
      "excellent 0.76683777378\n",
      "highly 0.684311342379\n",
      "best 0.616289500405\n",
      "movie 0.563574460608\n",
      "love 0.671274285483\n",
      "nano 0.517353692167\n",
      "deal -0.601048610939\n",
      "tried -0.59241464448\n",
      "allows 0.510326526292\n",
      "output 0.68343831325\n",
      "poor -0.730767022108\n",
      "flaw -0.521784990053\n",
      "returned -0.53275504713\n",
      "seemed -0.516735757038\n",
      "awesome 0.537543898842\n",
      "try -0.615384390137\n",
      "hope -0.558355045659\n",
      "perfect 0.608982158396\n",
      "return -0.718856182349\n",
      "unfortunately -0.677654923881\n",
      "waste -0.666326517673\n",
      "worst -0.500021492849\n",
      "terrible -0.681782309297\n"
     ]
    }
   ],
   "source": [
    "# let's look at the weights for each word\n",
    "# try it with different threshold values!\n",
    "threshold = 0.5\n",
    "for word, index in vocabulary.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
